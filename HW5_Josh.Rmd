---
title: "Modern Data Mining - HW 5"
author:
- Joshua Benadiva
- Christine O'Hara
- Mintai Bautista
date: 'Due: 11:59Pm,  4/16, 2023'
output:
  html_document:
    code_folding: show
    highlight: haddock
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height=4, fig.width=6, warning = F)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, data.table, randomForest, caret, tree, ISLR, rpart, rattle, pROC, partykit, ggplot2, glmnet, leaps, dplyr, keras, neuralnet, imager, ranger)
```

### Problem 1 - Yelp Data

## Reading in Data

a) Which time period were the reviews collected in this data?

2004 - 2018 

b) Are ratings (with 5 levels) related to month of the year or days of the week? Only address this through EDA please.

No - no obvious pattern in  ratings based on month or day of the week. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height=4, fig.width=6, warning = F)

if (!require("pacman")) install.packages("pacman")
pacman::p_load(randomForest, tree, ISLR, rpart, rattle, pROC, partykit, ggplot2, glmnet, leaps, dplyr, keras, neuralnet, imager, ranger, ggplot2, tm, SnowballC, RColorBrewer, wordcloud,
               randomForest, ranger, data.table, sparsepca)

yelp_data <- jsonlite::stream_in(file("yelp_review_20k.json"), verbose = F)
str(yelp_data)

# Examining year
yelp_data$year <- substr(yelp_data$date, 1, 4)
yelp_data$year
str(yelp_data)
yelp_data$year <- as.numeric(yelp_data$year)
str(yelp_data)
summary(yelp_data)
yelp_data$stars <- as.factor(yelp_data$stars)

# Examining month and day of week 
weekdays <- weekdays(as.Date(yelp_data$date))
months <- months(as.Date(yelp_data$date))

par(mfrow=c(1,2))
pie(table(weekdays), main="Prop of reviews") # Pretty much evenly distributed
pie(table(months))

prop.table(table(yelp_data$stars, weekdays), 2) # prop of the columns
prop.table(table(yelp_data$stars, weekdays), 1) # prop of the rows

prop.table(table(yelp_data$stars, months), 2) # prop of the columns
prop.table(table(yelp_data$stars, months), 1) # prop of the rows
```


## Document term matrix (dtm) (bag of words)
Extract document term matrix for texts to keep words appearing at least .5% of the time among all 20000 documents. Go through the similar process of cleansing as we did in the lecture.

a) Briefly explain what does this matrix record? What is the cell number at row 100 and column 405? What does it represent?

0 

b) What is the sparsity of the dtm obtained here? What does that mean?

97% 

c) Set the stars as a two category response variable called rating to be “1” = 5,4 and “0”= 1,2,3. Combine
the variable rating with the dtm as a data frame called data2

```{r}
data1.text <- yelp_data$text # take the text out summary(data)

# Turn texts to corpus
mycorpus1  <- VCorpus(VectorSource(data1.text))


# Control list for creating our DTM within DocumentTermMatrix
# Can tweak settings based off if you want punctuation, numbers, etc.
control_list <- list( tolower = TRUE, 
                      removePunctuation = TRUE,
                      removeNumbers = TRUE, 
                      stopwords = stopwords("english"), 
                      stemming = TRUE)
# dtm with all terms:
dtm.10.long  <- DocumentTermMatrix(mycorpus1, control = control_list)
#inspect(dtm.10.long)

# kick out rare words 
dtm.10<- removeSparseTerms(dtm.10.long, 1-.01)  
inspect(dtm.10)
as.matrix(dtm.10[100,405])

```


## LASSO
i. Use the training data to get Lasso fit. Choose lambda.1se. Label the the fit as fit.lasso. Comment on what tuning parameters are chosen at the end and why?

ii. Feed the output from Lasso above, get a logistic regression and call this fit.glm

```{r}
yelp_data$stars <- as.numeric(yelp_data$stars)
yelp_data$rating <- if_else(yelp_data$stars <= 3, 0 , 1, missing = NULL)
data2 <- data.frame(yelp_data$rating, as.matrix(dtm.10) )

set.seed(1)  # for the purpose of reproducibility
n <- nrow(data2)
test.num <- 5000
valid.num <-2000
test.valid.indexes <- sample(n, test.num + valid.num)
test.index <- test.valid.indexes[1:test.num] 
valid.index <- test.valid.indexes[(test.num + 1): (test.num + valid.num)]

data2.test <- data2[test.index,] 
data2.valid <- data2[valid.index,]
data2.train <- data2[-test.valid.indexes,]
dim(data2.train)
dim(data2.valid)
dim(data2.test)

#Run LASSO

y <- data2.train$yelp_data.rating
X1 <- sparse.model.matrix(yelp_data.rating~., data=data2.train)[, -1]
set.seed(2)
result.lasso <- cv.glmnet(X1, y, alpha=.99, family="binomial")
plot(result.lasso)
saveRDS(result.lasso, file="TextMining_lasso.RDS")
result.lasso <- readRDS("TextMining_lasso.RDS")


coef.1se <- coef(result.lasso, s="lambda.1se") 
lasso.words <- coef.1se@Dimnames[[1]] [coef.1se@i][-1] 
summary(lasso.words)  ## 401 lasso words

#Run logistic regression

sel_cols <- c("yelp_data.rating", lasso.words)
data_sub <- data2.train %>% select(all_of(sel_cols))
result.glm <- glm(yelp_data.rating~., family=binomial, data_sub)

#strip GLM 

stripGlmLR = function(cm) {
  cm$y = c()
  cm$model = c()
  
  cm$residuals = c()
  cm$fitted.values = c()
  cm$effects = c()
  cm$qr$qr = c()  
  cm$linear.predictors = c()
  cm$weights = c()
  cm$prior.weights = c()
  cm$data = c()

  
  cm$family$variance = c()
  cm$family$dev.resids = c()
  cm$family$aic = c()
  cm$family$validmu = c()
  cm$family$simulate = c()
  attr(cm$terms,".Environment") = c()
  attr(cm$formula,".Environment") = c()
  
  cm
}

result.glm.small <- stripGlmLR(result.glm)
#(result.glm.small$coefficients)

saveRDS(result.glm.small, 
     file = "TextMining_glm_small.RDS")

result.glm <- readRDS("TextMining_glm_small.RDS")
result.glm.coef <- coef(result.glm)
result.glm.coef[200:250]
hist(result.glm.coef)

```

## Create word cloud

a) Pull out all the positive coefficients and the corresponding words. Rank the coefficients in a decreasing order. Report the leading 2 words and the coefficients. Describe briefly the interpretation for those two coefficients.

delici - 1.5827174364 
knowledg - 1.3037323687

b) Make a word cloud with the top 100 positive words according to their coefficients. Interpret the cloud briefly.

Positive word cloud contains stems of words. Delici, profession, classic, recommend show up often

c) Repeat a) and b) above for the bag of negative words.

worst - 3.1593784430     
horribl - 2.3128257719

d) Summarize the findings.

Worst, horribl, bland, rude, poor show up often.

```{r setup, include=TRUE, cache = FALSE}

good.glm <- result.glm.coef[which(result.glm.coef > 0)]
good.glm <- good.glm[-1]  # took intercept out
names(good.glm)[1:20]  # which words are positively associated with good ratings

good.fre <- sort(good.glm, decreasing = TRUE) # sort the coef's
round(good.fre, 4)[1:20] # leading 20 positive words
length(good.fre)  # 196 good words

hist(as.matrix(good.fre), breaks=30, col="red") 
good.word <- names(good.fre)  # good words with a decreasing order in the coeff's

cor.special <- brewer.pal(8,"Dark2")  # set up a pretty color scheme
wordcloud(good.word[1:100], good.fre[1:100],  # make a word cloud
          colors=cor.special)

#Negative word cloud

bad.glm <- result.glm.coef[which(result.glm.coef < 0)]
pr

bad.fre <- sort(-bad.glm, decreasing = TRUE)
round(bad.fre, 4)[1:40]
length(bad.fre)

hist(as.matrix(bad.fre), breaks=30, col="green")
bad.word <- names(bad.fre)
length(bad.word)
print(bad.word)
wordcloud(bad.word[1:100], bad.fre[1:100], min.freq=0, 
          color=cor.special)

```

## Comparing the models 

iii. What are the major differences among the two methods used so far: Lasso and glm

iv. Using majority votes find the testing errors 

a) From fit.lasso --> 0.14
b) From fit.glm  --> 0.18
c) Which one is smaller? --> LASSO is smaller


```{r}
predict.glm <- predict(result.glm, data2.test, type = "response")
class.glm <- ifelse(predict.glm > .5, "1", "0")
length(class.glm)

testerror.glm <- mean(data2.test$yelp_data.rating != class.glm)
testerror.glm   # mis classification error is 0.18

pROC::roc(data2.test$yelp_data.rating, predict.glm, plot=T) # AUC=.87

predict.lasso.p <- predict(result.lasso, as.matrix(data2.test[, -1]), type = "response", s="lambda.1se")
  # output lasso estimates of prob's
predict.lasso <- predict(result.lasso, as.matrix(data2.test[, -1]), type = "class", s="lambda.1se")
  # output majority vote labels

# LASSO testing errors
mean(data2.test$yelp_data.rating != predict.lasso)   # .14

# ROC curve for LASSO estimates is 0.93

pROC::roc(data2.test$yelp_data.rating, predict.lasso.p, plot=TRUE)

```

## Neural network

*could not get this to work* 

i. Let’s specify an architecture with the following specifications
a) One hidden layers with 20 neurons
b) Relu activation function
c) Softmax output
d) Explain in a high level what is the model? How many unknown weights (parameters are there)

ii. Train your model and call it fit.nn 
a) using the training data
b) split 85% vs. 15% internally
c) find the optimal epoch

iii. Report the testing errors using majority vote.


```{r}

devtools::install_github("rstudio/keras")
library(keras)
install_keras()

data3 <- data2[, -c(1:3)]; dim(data3)# the first element is the rating
names(data3)[1:3]
levels(as.factor(data3$rating))

set.seed(1)  # for the purpose of reproducibility
n <- nrow(data3)
.index <- sample(n, 10000)
length(.index)   # reserve 10000
data3_val <- data3[.index, ] # 
##  input/y
data3_xval <- as.matrix(data3_val[, -1])  # make sure it it is a matrix
data3_yval <- as.matrix(data3_val[, 1]) # make sure it it is a matrix

data3_xtrain <- data3[-.index, -1]   #dim(data3_xtrain)
data3_ytrain <- data3[-.index, 1]   
data3_xtrain <- as.matrix(data3_xtrain) # make sure it it is a matrix
data3_ytrain <- as.matrix(data3_ytrain) # make sure it it is a matrix

p <- dim(data3_xtrain)[2] # number of input variables
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(p)) %>% 
  # 1 layer with 16 neurons. default activation is relu
  layer_dense(units = 8, activation = "relu") %>%  
  # layer 2 with 8 neurons
  layer_dense(units = 2, activation = "softmax") # output
print(model)

model %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = c("accuracy")
)

#retain the nn:
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(p)) %>% 
  # 1 layer with 16 neurons. default activation is relu
  layer_dense(units = 8, activation = "relu") %>%  # layer 2 with 8 neurons
  layer_dense(units = 2, activation = "softmax") # output

model %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = c("accuracy")
)

 model %>% fit(data3_xtrain, data3_ytrain, epochs = 6, batch_size = 512)
 
 results <- model %>% evaluate(data3_xval, data3_yval) ; 
results
```

## Random Forest

i. Briefly summarize the method of Random Forest
ii. Now train the data using the training data set by RF and call it fit.rf.

>Random forest is a commonly-used machine learning algorithm, which combines the output of multiple decision trees to reach a single result. It is flexible enough to handle both classification and regression problems.

a) Explain how you tune the tuning parameters (mtry and ntree)

>mtry: Number of variables randomly selected as testing conditions at each split of decision trees. default value is sqr(col). Increasing mtry generally improves the performance of the model as each node has a higher number of options to be considered.

>ntree: Number of trees to grow. the default value is 500. A higher number of trees give you better performance but makes your code slower. 

b) Get the testing error of majority vote.

> this gives 14% testing error

```{r}
fit.rf <- ranger::ranger(yelp_data.rating~., data2.train, num.trees = 200, importance="impurity")
fit.rf
imp <- importance(fit.rf)
imp[order(imp, decreasing = T)][1:20]

predict.rf <- predict(fit.rf, data=data2.test, type="response")
mean(data2.test$yelp_data.rating != predict.rf$predictions)
```

## PCA

i. Perform PCA (better to do sparse PCA) for the input matrix first. Decide how many PC’s you may want to take and why.

ii. Pick up one of your favorite method above and build the predictive model with PC’s. Say you use RandomForest.

iii. What is the testing error? Is this testing error better than that obtained using the original x’s?

>Testing error is 26%. This is worse than using the original X's. 

```{r}
set.seed(10)
data1 <- sample_n(data2, size=2000) 
dim(data1)
str(data1 [820:826])
lapply(data1, as.numeric)


# Get train/test data 

set.seed(1)
n=nrow(data1)
test.index <- sample(n, 0.3*n)
length(test.index)
data2.test <- data1[test.index, ] 
data2.train <- data1[-test.index, ]
names(data2.train)[1:3]
dim(data2.train)
str(data2.train)

#Perform PCA

pc.train <- prcomp(data2.train[, -c(1)])  # Take the rating out

pc.train$center[1:50]
hist(pc.train$center, breaks=50,
     col="blue",
     main="mean frequency of the words")  

pc.train.imp <- t((summary(pc.train))$importance)   
pc.train.imp <- as.data.frame(pc.train.imp) 
names(pc.train.imp) <- c("Sdev", "PVE", "CPVE")
attach(pc.train.imp)
par(mfrow=c(3,1))
hist(Sdev)
plot(PVE, xlim=c(1, 50))
plot(CPVE, main="Scree plot of CPVE")  
detach(pc.train.imp)


# Extract PC scores
pc.train.scores <- pc.train$x
dim(pc.train.scores)

pc.test.scores <- predict(pc.train, data2.test[, -c(1)]) 
dim(pc.test.scores)

#Perform LASSO with PCA scores

y <- data2.train$yelp_data.rating
X <- as.matrix(pc.train.scores)    # pc scores for training data
pca.lasso <- cv.glmnet(X, y, alpha=0, family="binomial")  
plot(pca.lasso)
beta.lasso <- coef(pca.lasso, s="lambda.min")   
beta <- beta.lasso[which(beta.lasso !=0),] 
beta <- as.matrix(beta);
beta <- rownames(beta)
beta[1:20]   

#Testing Errors for LASSO w/ PCA 

predict.glm <- predict(pca.lasso, pc.test.scores, type = "response")
class.glm <- rep("0", nrow(data2.test))
class.glm[predict.glm > .5] ="1"
length(class.glm)

testerror.glm <- mean(data2.test$yelp_data.rating != class.glm)
testerror.glm
pROC::roc(data2.test$yelp_data.rating, predict.glm, plot=T)

```


## Ensemble model
i. Take average of some of the models built above (also try all of them) and this gives us the fit.em.

Report its testing error. (Do you have more models to be bagged, try it.)

```{r}
library(tidyr)

#create an ensemble model 

data4.test <- data2.test

data4.test$glm_pred <- predict(
result.glm,
  newdata = data2.test
)

data4.test$lassopca_pred <- predict(
pca.lasso, newx = pc.test.scores,
  newdata = data2.test
)

X1 <- sparse.model.matrix(yelp_data.rating~., data=data2.test)[, -1]

data4.test$lasso_pred <- predict(
result.lasso, newx = X1,
  newdata = data2.test
)

data4.test$ranger_pred <- predict(
fit.rf,
  data = data2.test
) 

data4.test$lasso_pred <- as.numeric(data4.test$lasso_pred)
data4.test$lassopca_pred <- as.numeric(data4.test$lassopca_pred)
data4.test$glm_pred <- as.numeric(data4.test$glm_pred)

fit.em <- data4.test %>% 
  mutate(equal_weight_pred = (glm_pred + lassopca_pred+ lasso_pred) / 3)

#final model testing errors --> couldn't get this to work with TA

fit.em
mean(data2.valid$yelp_data.rating != fit.em$equal_weight_pred) 

```


## Final model
Which classifier(s) seem to produce the least testing error? Are you surprised? 

>Couldn't get this to work

Report the final model and accompany the validation error. Once again this is THE only time you use the validation data set.

> Couldn't get this to work

For the purpose of prediction, comment on how would you predict a rating if you are given a review (not a tm output) using our final model?

> Without a model, I would scan the review for positive and negative words to predict the rating. 


# Problem 2: IQ and successes

## Background: Measurement of Intelligence 

Case Study:  how intelligence relates to one's future successes?



ASVAB (Armed Services Vocational Aptitude Battery) tests have been used as a screening test for those who want to join the army or other jobs. 

Our data set IQ.csv is a subset of individuals from the 1979 National Longitudinal Study of 
Youth (NLSY79) survey who were re-interviewed in 2006. Information about family, personal demographic such as gender, race and education level, plus a set of ASVAB (Armed Services Vocational Aptitude Battery) test scores are available. It is STILL used as a screening test for those who want to join the army! ASVAB scores were 1981 and income was 2005. 

**Our goals:** 

+ Is IQ related to one's successes measured by Income?
+ Is there evidence to show that Females are under-paid?
+ What are the best possible prediction models to predict future income? 


**The ASVAB has the following components:**

+ Science, Arith (Arithmetic reasoning), Word (Word knowledge), Parag (Paragraph comprehension), Numer (Numerical operation), Coding (Coding speed), Auto (Automative and Shop information), Math (Math knowledge), Mechanic (Mechanic Comprehension) and Elec (Electronic information).
+ AFQT (Armed Forces Qualifying Test) is a combination of Word, Parag, Math and Arith.
+ Note: Service Branch requirement: Army 31, Navy 35, Marines 31, Air Force 36, and Coast Guard 45,(out of 100 which is the max!) 

**The detailed variable definitions:**

Personal Demographic Variables: 

 * Race: 1 = Hispanic, 2 = Black, 3 = Not Hispanic or Black
 * Gender: a factor with levels "female" and "male"
 * Educ: years of education completed by 2006
 
Household Environment: 
 
* Imagazine: a variable taking on the value 1 if anyone in the respondent’s household regularly read
	magazines in 1979, otherwise 0
* Inewspaper: a variable taking on the value 1 if anyone in the respondent’s household regularly read
	newspapers in 1979, otherwise 0
* Ilibrary: a variable taking on the value 1 if anyone in the respondent’s household had a library card
	in 1979, otherwise 0
* MotherEd: mother’s years of education
* FatherEd: father’s years of education

Variables Related to ASVAB test Scores in 1981 (Proxy of IQ's)

* AFQT: percentile score on the AFQT intelligence test in 1981 
* Coding: score on the Coding Speed test in 1981
* Auto: score on the Automotive and Shop test in 1981
* Mechanic: score on the Mechanic test in 1981
* Elec: score on the Electronics Information test in 1981

* Science: score on the General Science test in 1981
* Math: score on the Math test in 1981
* Arith: score on the Arithmetic Reasoning test in 1981
* Word: score on the Word Knowledge Test in 1981
* Parag: score on the Paragraph Comprehension test in 1981
* Numer: score on the Numerical Operations test in 1981

Variable Related to Life Success in 2006

* Income2005: total annual income from wages and salary in 2005. We will use a natural log transformation over the income.


**Note: All the Esteem scores shouldn't be used as predictors to predict income**

## 1. EDA: Some cleaning work is needed to organize the data. 
```{r}
iq_data <- fread("IQ.Full.csv")
print(iq_data)
```

+ The first variable is the label for each person. Take that out.


+ Set categorical variables as factors. 
+ Make log transformation for Income and take the original Income out
+ Take the last person out of the dataset and label it as **Michelle**. 
+ When needed, split data to three portions: training, testing and validation (70%/20%/10%)
  - training data: get a fit
  - testing data: find the best tuning parameters/best models
  - validation data: only used in your final model to report the accuracy. 

```{r}
iq_data <- iq_data[, -1]
iq_data$Race <- as.factor(iq_data$Race)
iq_data$Gender <- as.factor(iq_data$Gender)
iq_data$LogIncome2005 <- log(iq_data$Income2005)
iq_data <- subset(iq_data, select = -Income2005)
michelle <- iq_data[nrow(iq_data), ]
iq_data <- iq_data[-nrow(iq_data), ]
set.seed(123) # Set a seed for reproducibility
total_rows <- nrow(iq_data)
train_rows <- floor(total_rows * 0.7)
test_rows <- floor(total_rows * 0.2)

# Shuffle data
shuffled_data <- iq_data[sample(nrow(iq_data)), ]

# Split data
train_data <- shuffled_data[1:train_rows, ]
test_data <- shuffled_data[(train_rows + 1):(train_rows + test_rows), ]
validation_data <- shuffled_data[(train_rows + test_rows + 1):total_rows, ]
```

## 2. Factors affect Income

We only use linear models to answer the questions below.

i. To summarize ASVAB test scores, create PC1 and PC2 of 10 scores of ASVAB tests and label them as
ASVAB_PC1 and ASVAB_PC2. Give a quick interpretation of each ASVAB_PC1 and ASVAB_PC2 in terms of the original 10 tests. 

```{r}
# Extract ASVAB test scores
asvab_scores <- iq_data[, c("Science", "Arith", "Word", "Parag", "Numer", "Coding", "Auto", "Math", "Mechanic", "Elec")]

# Perform PCA
asvab_pca <- prcomp(asvab_scores, scale = TRUE)

# Add PC1 and PC2 to the dataset
iq_data$ASVAB_PC1 <- asvab_pca$x[, 1]
iq_data$ASVAB_PC2 <- asvab_pca$x[, 2]
asvab_pca$rotation[, 1:2]

```

Based on the PCA results for the ASVAB test scores, PC1 and PC2 capture different aspects of the test scores.

PC1:
PC1 has positive loadings for all 10 tests, which suggests that it represents a general aptitude or overall ability level. This means that individuals who score high on PC1 tend to perform well across all tests, while those who score low on PC1 perform poorly across all tests. A high value of ASVAB_PC1 indicates strong performance in all areas, reflecting a more versatile skill set.

PC2:
PC2, on the other hand, shows a mix of positive and negative loadings. The tests with positive loadings are Arith, Word, Parag, Numer, Coding, and Math. The tests with negative loadings are Science, Auto, Mechanic, and Elec. PC2 seems to capture the contrast between verbal/numerical skills (positive loadings) and technical/mechanical skills (negative loadings). A high value of ASVAB_PC2 indicates a stronger performance in verbal and numerical skills and a weaker performance in technical and mechanical skills, whereas a low value indicates the opposite.

ii. Is there any evidence showing ASVAB test scores in terms of ASVAB_PC1 and ASVAB_PC2, might affect the Income?  Show your work here. You may control a few other variables, including gender. 

```{r}
income_model <- lm(LogIncome2005 ~ ASVAB_PC1 + ASVAB_PC2 + Gender, data = iq_data)
summary(income_model)
```
ASVAB_PC1: The coefficient for ASVAB_PC1 is 0.109464, with a highly significant p-value (< 2e-16). This indicates that there is a positive relationship between the first principal component of ASVAB test scores (general aptitude) and income. As an individual's general aptitude (ASVAB_PC1) increases, their income also tends to increase.

ASVAB_PC2: The coefficient for ASVAB_PC2 is 0.094176, with a significant p-value (2.61e-07). This suggests a positive relationship between the second principal component of ASVAB test scores (verbal and numerical skills vs. technical and mechanical skills) and income. As an individual's verbal and numerical skills relative to their technical and mechanical skills (ASVAB_PC2) increase, their income also tends to increase.

Gender: The coefficient for Gendermale is 0.671353, with a highly significant p-value (< 2e-16). This indicates that, on average, male individuals tend to have higher incomes than female individuals.

The model has an adjusted R-squared of 0.1919, which implies that about 19.19% of the variation in income can be explained by the ASVAB_PC1, ASVAB_PC2, and gender variables. While this is not a very high value, it does suggest a significant relationship between ASVAB test scores, gender, and income.


iii. Is there any evidence to show that there is gender bias against either male or female in terms of income in the above model? 

Yes, there is evidence of gender bias in terms of income in the given model. The coefficient for Gendermale is 0.671353, with a highly significant p-value (< 2e-16). This result suggests that, on average, male individuals tend to have higher incomes than female individuals, even when controlling for ASVAB_PC1 and ASVAB_PC2 scores.

We next build a few models for the purpose of prediction using all the information available. From now on you may use the three data sets setting (training/testing/validation) when it is appropriate. 

## 3. Trees

i. fit1: tree(Income ~ Educ + Gender, data.train) with default set up 

```{r}
library(tree)

fit1 <- tree(LogIncome2005 ~ Educ + Gender, data = train_data)
```

    a) Display the tree
    
```{r}
plot(fit1)
text(fit1, pretty = 0)
```

    b) How many end nodes? Briefly explain how the estimation is obtained in each end nodes and deescribe the prediction equation
```{r}
length(unique(fit1$where))
```


The estimation in each end node is obtained by calculating the average LogIncome2005 for all observations within that node. When making predictions using this decision tree, the input observation will traverse the tree following the binary splits until it reaches an end node. The predicted value for that observation will be the average LogIncome2005 of the samples within the end node.

The prediction equation is not a single equation like in linear regression. Instead, it is a set of rules or binary decisions based on the input features (Educ and Gender) that guide the observation through the tree structure until it reaches an end node, where the average LogIncome2005 of that node is used as the predicted value.
For the left branch (Female):

The split after gender is based on Educ < 15.5:
Left end node (terminal node): Educ < 15.5, predicted LogIncome2005 = 9.959
Right end node (terminal node): Educ >= 15.5, predicted LogIncome2005 = 10.520
For the right branch (Male):

The split after gender is based on Educ < 15.5:
Left end node (terminal node): Educ < 15.5, predicted LogIncome2005 = 10.570
Right end node (terminal node): Educ >= 15.5, predicted LogIncome2005 = 11.160

    c) Does it show interaction effect of Gender and Educ over Income?

 In the tree, the first split is based on Gender, and after that, it splits based on Educ. Since the tree does not split on Educ first and then further splits on Gender or vice versa within each branch, it suggests that the interaction effect between Gender and Educ over Income is not strong, at least within the context of this tree.

    d) Predict Michelle's income
```{r}
michelle_pred <- predict(fit1, newdata = michelle)
exp(michelle_pred)  # Convert back to original income scale
```

ii. fit2: fit2 <- rpart(Income2005 ~., data.train, minsplit=20, cp=.009)

```{r}
fit2 <- rpart(LogIncome2005 ~ ., data = train_data, minsplit = 20, cp = 0.009)
```


    a) Display the tree using plot(as.party(fit2), main="Final Tree with Rpart") 
```{r}
plot(as.party(fit2), main = "Final Tree with Rpart")
```

    b) A brief summary of the fit2
```{r}
summary(fit2)
```
    c) Compare testing errors between fit1 and fit2. Is the training error from fit2 always less than that from fit1? Is the testing error from fit2 always smaller than that from fit1? 

```{r}
# Predictions for fit1 and fit2
pred_fit1 <- predict(fit1, newdata = test_data)
pred_fit2 <- predict(fit2, newdata = test_data)

# Calculate the Mean Squared Error (MSE) for both models
mse_fit1 <- mean((test_data$LogIncome2005 - pred_fit1)^2)
mse_fit2 <- mean((test_data$LogIncome2005 - pred_fit2)^2)

# Display the MSE for both models
cat("MSE for fit1:", mse_fit1, "\n")
cat("MSE for fit2:", mse_fit2, "\n")
```

Is the training error from fit2 always less than that from fit1?
It's not guaranteed that the training error from fit2 will always be less than that from fit1. It depends on the complexity of the models and the specific data we're working with. In general, a more complex model may have a smaller training error, but it could also be more prone to overfitting.

Is the testing error from fit2 always smaller than that from fit1?
Again, there's no guarantee that the testing error for fit2 will always be smaller than that for fit1. It depends on how well each model generalizes to unseen data. A model with better generalization will have a smaller testing error.

Based on the results, the testing error for fit2 is smaller than that for fit1. This indicates that fit2 generalizes better to the unseen data in our test dataset than fit1. In this specific case, using fit2 (the model created with rpart) would likely provide better predictions for new data compared to fit1 (the model created with the tree package).
    
    
`    d) You may prune the fit2 to get a tree with small testing error. 
```{r}
library(rpart)
control_params <- rpart.control(minsplit = 20, cp = 0.009, xval = 10)  # 10-fold cross-validation

fit2_cv <- rpart(LogIncome2005 ~ ., data = train_data, control = control_params)
optimal_cp <- fit2_cv$cptable[which.min(fit2_cv$cptable[, "xerror"]), "CP"]
pruned_fit2 <- prune(fit2, cp = optimal_cp)
plot(as.party(pruned_fit2), main = "Pruned Tree with Rpart")
```


iii. fit3: bag two trees

    a) Take 2 bootstrap training samples and build two trees using the 
    rpart(Income2005 ~., data.train.b, minsplit=20, cp=.009). Display both trees.
```{r}
set.seed(123)  # Set a seed for reproducibility

# Create two bootstrap samples of the training data
train_data_b1 <- train_data[sample(nrow(train_data), replace = TRUE), ]
train_data_b2 <- train_data[sample(nrow(train_data), replace = TRUE), ]
fit3_tree1 <- rpart(LogIncome2005 ~ ., data = train_data_b1, minsplit = 20, cp = 0.009)
fit3_tree2 <- rpart(LogIncome2005 ~ ., data = train_data_b2, minsplit = 20, cp = 0.009)

plot(as.party(fit3_tree1), main = "Bootstrap Tree 1 with Rpart")
plot(as.party(fit3_tree2), main = "Bootstrap Tree 2 with Rpart")
```
    b) Explain how to get fitted values for Michelle by bagging the two trees obtained above. Do not use the predict().
    Traverse the first tree (fit3_tree1) using Michelle's data and find the terminal node where her data point falls. This can be done by following the splits based on the values of her predictor variables.

Retrieve the mean LogIncome2005 value for the terminal node in the first tree where Michelle's data point falls.

Repeat steps 1 and 2 for the second tree (fit3_tree2).

Average the mean LogIncome2005 values from both trees' terminal nodes to obtain the final prediction for Michelle's LogIncome2005.

To get Michelle's predicted income, take the exponent of the final LogIncome2005 prediction to convert it back to the original income scale.

    c) What is the testing error for the bagged tree. Is it guaranteed that the testing error by bagging the two tree always smaller that either single tree?

```{r}
pred_tree1 <- predict(fit3_tree1, newdata = test_data)
pred_tree2 <- predict(fit3_tree2, newdata = test_data)

# Average predictions to obtain bagged predictions
bagged_pred <- (pred_tree1 + pred_tree2) / 2

# Calculate the mean squared error
mse_bagged <- mean((test_data$LogIncome2005 - bagged_pred)^2)
print(mse_bagged)
```

No, it's not guaranteed that the testing error for the bagged trees will always be smaller than the error from either single tree. However, bagging can help improve model stability and reduce overfitting by averaging the predictions from multiple trees. This can often lead to better generalization and a lower testing error. The effectiveness of bagging depends on the specific problem, the dataset, and the underlying decision trees.
    
iv. fit4: Build a best possible RandomForest
    a) Show the process how you tune mtry and number of trees. Give a very high level explanation how fit4 is built.

```{r results=TRUE}
set.seed(555)
fit4 <- randomForest(LogIncome2005 ~ ., data = train_data, mtry = 5, ntree = 500)
names(fit4)
plot(fit4, col="red", pch=16, type="p", 
     main="default plot, ")
```

Before we can explain all the output. We first get the predicted values and training error.
```{r}
# Obtain predicted values for the training set
train_pred <- predict(fit4, newdata = train_data)

# Calculate the mean squared error for the training set
mse_train <- mean((train_data$LogIncome2005 - train_pred)^2)

# Print the mean squared error
print(mse_train)
```

Now to see th oob.times and store the predicted values under the predicted oob, and store the mean squared error.

```{r}
head(fit4$oob.times) # how many times each obs'n belong to OOB. We expect to see 1/e=1/3 (.37)

predicted_oob <- fit4$predicted   # OOB predicted values 
mse_oob <- mean((train_data$LogIncome2005 - predicted_oob)^2)  
```

<!-- Notice `mse_oob`=`r mse_oob` is much larger than that `mse_train`= `r mse_train`. -->


```{r}
plot(fit4$mse, xlab="number of trees", col="blue",
     ylab="ave mse up to i many trees using OOB predicted",
     pch=16) # We only need about 100 trees for this
title(main = "OOB testing errors as a func of number of trees")
```
Now we fix `ntree=250`, We only want to compare the OOB mse[250] to see the mtry effects.
Here we loop mtry from 1 to 19 and return the testing OOB errors

```{r}
rf.error.p <- 1:19  # set up a vector of length 19
for (p in 1:19)  # repeat the following code inside { } 19 times
{
  fit5 <- randomForest(LogIncome2005 ~ ., data = train_data, mtry = 5, ntree = 250)
  #plot(fit5, col= p, lwd = 3)
  rf.error.p[p] <- fit5$mse[250]  # collecting oob mse based on 250 trees
}
rf.error.p   # oob mse returned: should be a vector of 19

plot(1:19, rf.error.p, pch=16,
     main = "Testing errors of mtry with 250 trees",
     xlab="mtry",
     ylab="OOB mse of mtry")
lines(1:19, rf.error.p)
```

So we see here that the mtry should be 16, although the mean squared error is quite stable. 

```{r}
plot(fit5, col="red", pch=16, type="p", 
     main="default plot, ")
```

The final fit: we take mtry = 16
```{r}
fit.final<- randomForest(LogIncome2005 ~ ., data = train_data, mtry = 16, ntree = 250)    # change mtry
plot(fit.final)
fit.final$mse[250]  # testing error of the RF based on 250 trees
```
    
High-level explanation of how the RandomForest model (fit.final) is built:

In this analysis, I built a random forest model to predict LogIncome2005 using the given training data. Here's a high-level explanation of the steps I took:

I first created a random forest model fit4 with mtry = 5 and ntree = 500. I plotted the default output of the model to visualize its performance.

I calculated the training error (MSE) for fit4 by obtaining the predicted values for the training set and comparing them to the actual LogIncome2005 values.

I examined the out-of-bag (OOB) information provided by the random forest model, which gives me an unbiased estimation of the testing error. I retrieved the OOB predicted values and calculated the mean squared error for the OOB predictions.

I plotted the OOB testing errors as a function of the number of trees and observed that around 100 trees were sufficient for this problem.

To find the optimal mtry value, I fixed ntree = 250 and looped through mtry values from 1 to 19. For each mtry value, I built a random forest model and collected the OOB MSE based on 250 trees. I plotted the OOB MSE for each mtry value and observed that mtry = 16 appeared to be the best choice, though the mean squared error was quite stable across different mtry values.

Finally, I built the final random forest model fit.final with mtry = 16 and ntree = 250. I plotted the default output for this model and reported the testing error (MSE) based on 250 trees.

In summary, I built a random forest model to predict LogIncome2005, tuned the hyperparameters mtry and ntree using OOB testing errors, and chose the optimal values for these parameters to construct the final model.

    b) Compare the oob errors form fit4 (I called it fit.final) to the testing errors using your testing data. Are you convinced that oob errors estimate testing error reasonably well.

```{r}
# Obtain predicted values for the testing set
test_pred <- predict(fit.final, newdata = test_data)

# Calculate the mean squared error for the testing set
mse_test <- mean((test_data$LogIncome2005 - test_pred)^2)

# Print the mean squared error
print(mse_test)
```
 I am convinced that oob errors estimate testing errors reasonably well. 
 
    c) What is the predicted value for Michelle?

```{r}

# Remove the LogIncome2005 column for prediction
michelle_data <- michelle[-which(colnames(iq_data) == "LogIncome2005")]

# Predict LogIncome2005 for Michelle
michelle_pred <- predict(fit.final, newdata = michelle_data)

# Print the predicted value
print(michelle_pred)
```
    
v. Now you have built so many predicted models (fit1 through fit4 in this section). What about build a fit5 which bags fit1 through fit4. Does fit5 have the smallest testing error?

```{r}
# Obtain predictions for the test set using individual models
pred_fit1 <- predict(fit1, newdata = test_data)
pred_fit2 <- predict(fit2, newdata = test_data)
pred_fit3_tree1 <- predict(fit3_tree1, newdata = test_data)
pred_fit3_tree2 <- predict(fit3_tree2, newdata = test_data)
pred_fit3_bagged <- (pred_fit3_tree1 + pred_fit3_tree2) / 2
pred_fit_final <- predict(fit.final, newdata = test_data)

# Average predictions to obtain bagged predictions
bagged_pred_fit5 <- (pred_fit1 + pred_fit2 + pred_fit3_bagged + pred_fit_final) / 4

# Calculate the mean squared error for fit5
mse_fit5 <- mean((test_data$LogIncome2005 - bagged_pred_fit5)^2)
print(mse_fit5)
```

vi.  Summarize the results and nail down one best possible final model you will recommend to predict income. Explain briefly why this is the best choice. Finally for the first time evaluate the prediction error using the validating data set.

fit5, the bagged prediction of fit1, fit2, fit3 and fit.final, seemed to have the lowest mean squared error. This makes it the best choice for model selection. 

vii. Use your final model to predict Michelle's income. 
```{r}
# Obtain predicted value for Michelle using the final model (fit.final)
michelle_pred_final <- predict(fit5, newdata = michelle)

# Convert the predicted log income back to the original income scale
michelle_income_final <- exp(michelle_pred_final)

# Print the predicted income for Michelle
print(michelle_income_final)
```





